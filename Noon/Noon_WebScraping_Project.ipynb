{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67cmT8spZw5G",
    "outputId": "f60d4771-f7f6-42ba-fcea-40546fe6d745"
   },
   "outputs": [],
   "source": [
    "#!pip install selenium\n",
    "#!pip install pyvirtualdisplay\n",
    "#!apt-get install xvfb\n",
    "#!pip install undetected-chromedriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Home_Appliances] term='vacuum cleaner' variant='vacuum cleaner' page 1/10 -> 50 rows appended (pages_left_for_term=9)\n",
      "[Home_Appliances] term='vacuum cleaner' variant='vacuum cleaner' page 2/10 -> 54 rows appended (pages_left_for_term=8)\n",
      "[Home_Appliances] term='vacuum cleaner' variant='vacuum cleaner' page 3/10 -> 54 rows appended (pages_left_for_term=7)\n",
      "[Home_Appliances] term='vacuum cleaner' variant='vacuum cleaner' page 4/10 -> 54 rows appended (pages_left_for_term=6)\n",
      "[Home_Appliances] term='vacuum cleaner' variant='vacuum cleaner' page 5/10 -> 54 rows appended (pages_left_for_term=5)\n",
      "[Home_Appliances] term='vacuum cleaner' variant='vacuum cleaner' page 6/10 -> 54 rows appended (pages_left_for_term=4)\n",
      "[Home_Appliances] term='vacuum cleaner' variant='vacuum cleaner' page 7/10 -> 54 rows appended (pages_left_for_term=3)\n",
      "[Home_Appliances] term='vacuum cleaner' variant='vacuum cleaner' page 8/10 -> 54 rows appended (pages_left_for_term=2)\n",
      "[Home_Appliances] term='vacuum cleaner' variant='vacuum cleaner' page 9/10 -> 54 rows appended (pages_left_for_term=1)\n",
      "[Home_Appliances] term='vacuum cleaner' variant='vacuum cleaner' page 10/10 -> 54 rows appended (pages_left_for_term=0)\n",
      "[TV_and_Screens] term='smart tv' variant='smart tv' page 1/10 -> 53 rows appended (pages_left_for_term=9)\n",
      "[TV_and_Screens] term='smart tv' variant='smart tv' page 2/10 -> 53 rows appended (pages_left_for_term=8)\n",
      "[TV_and_Screens] term='smart tv' variant='smart tv' page 3/10 -> 53 rows appended (pages_left_for_term=7)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load: https://www.noon.com/egypt-en/search/?page=4&q=smart+tv. Last error: Message: invalid session id\nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0x11512d3\n\t0x1151314\n\t0xf3e52b\n\t0xf7cf57\n\t0xf7e1d4\n\t0x13a5314\n\t0x13a08cb\n\t0x13bd1aa\n\t0x116b1d8\n\t0x11731dd\n\t0x11595d8\n\t0x1159799\n\t0x1143b28\n\t0x76e45d49\n\t0x76fad5db\n\t0x76fad561\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 391\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;66;03m# headless=False عشان تشوفي اللي بيحصل، لو عايزة headless خليها True\u001b[39;00m\n\u001b[1;32m--> 391\u001b[0m     \u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 360\u001b[0m, in \u001b[0;36mrun_all\u001b[1;34m(headless)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    359\u001b[0m url \u001b[38;5;241m=\u001b[39m BASE_SEARCH\u001b[38;5;241m.\u001b[39mformat(page\u001b[38;5;241m=\u001b[39mp, query\u001b[38;5;241m=\u001b[39mquote_plus(query_variant))\n\u001b[1;32m--> 360\u001b[0m rows, _html, ok \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_rows_for_page\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_term\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_term\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_variant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_variant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage_no\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[0;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] term=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_term\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m variant=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_variant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpages_for_this_variant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> 0 products (stop this variant)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 262\u001b[0m, in \u001b[0;36mfetch_rows_for_page\u001b[1;34m(driver, url, category, search_group, search_term, query_variant, page_no, tries)\u001b[0m\n\u001b[0;32m    259\u001b[0m last_html \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, tries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 262\u001b[0m     last_html \u001b[38;5;241m=\u001b[39m \u001b[43mget_page_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     last_rows \u001b[38;5;241m=\u001b[39m parse_products(last_html, category, search_group, search_term, query_variant, page_no)\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(last_rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[8], line 154\u001b[0m, in \u001b[0;36mget_page_with_retry\u001b[1;34m(driver, url, retries, base_wait)\u001b[0m\n\u001b[0;32m    150\u001b[0m         last_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m    152\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.5\u001b[39m \u001b[38;5;241m*\u001b[39m attempt)\n\u001b[1;32m--> 154\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Last error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to load: https://www.noon.com/egypt-en/search/?page=4&q=smart+tv. Last error: Message: invalid session id\nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0x11512d3\n\t0x1151314\n\t0xf3e52b\n\t0xf7cf57\n\t0xf7e1d4\n\t0x13a5314\n\t0x13a08cb\n\t0x13bd1aa\n\t0x116b1d8\n\t0x11731dd\n\t0x11595d8\n\t0x1159799\n\t0x1143b28\n\t0x76e45d49\n\t0x76fad5db\n\t0x76fad561\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from urllib.parse import urljoin, quote_plus\n",
    "from datetime import date\n",
    "\n",
    "import undetected_chromedriver as uc\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# =======================\n",
    "# ✅ ONLY THIS LIST\n",
    "# =======================\n",
    "SEARCH_MAP = {\n",
    "      \n",
    "\n",
    "  \n",
    "\n",
    "    \"Home_Appliances\": [\n",
    "       [\"vacuum cleaner\"]\n",
    "    ],\n",
    "\n",
    "    \"TV_and_Screens\": [\n",
    "        [\"smart tv\"], [\"tv\"], [\"led tv\"], [\"uhd tv\"], [\"android tv\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Noon Settings\n",
    "# =======================\n",
    "BASE = \"https://www.noon.com\"\n",
    "BASE_SEARCH = \"https://www.noon.com/egypt-en/search/?page={page}&q={query}\"\n",
    "\n",
    "# =======================\n",
    "# Output: one file per day in Data/\n",
    "# =======================\n",
    "DATA_DIR = \"Data\"\n",
    "TODAY_STR = date.today().isoformat()  # YYYY-MM-DD\n",
    "OUT_CSV = os.path.join(DATA_DIR, f\"{TODAY_STR}.csv\")\n",
    "\n",
    "# =======================\n",
    "# ✅ LIMIT: max pages per SEARCH TERM (e.g., oil = max 10 pages total)\n",
    "# =======================\n",
    "MAX_PAGES_PER_TERM = 10\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Helpers\n",
    "# =======================\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def safe_text(el):\n",
    "    return el.get_text(strip=True) if el else None\n",
    "\n",
    "def clean_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "def detect_total_pages_from_html(html: str) -> int:\n",
    "    \"\"\"\n",
    "    أدق من جمع كل page= من الصفحة: يحاول يقرأ أرقام pagination فقط.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    nums = []\n",
    "\n",
    "    for a in soup.select('a[href*=\"page=\"]'):\n",
    "        href = a.get(\"href\", \"\")\n",
    "        m = re.search(r\"[?&]page=(\\d+)\", href)\n",
    "        if m:\n",
    "            nums.append(int(m.group(1)))\n",
    "\n",
    "    # fallback\n",
    "    if not nums:\n",
    "        for m in re.finditer(r\"[?&]page=(\\d+)\", html):\n",
    "            try:\n",
    "                nums.append(int(m.group(1)))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return max(nums) if nums else 1\n",
    "\n",
    "def detect_stock(txt: str):\n",
    "    \"\"\"\n",
    "    listing page مش دايمًا بيعرض stock صريح، فده best-effort detection\n",
    "    \"\"\"\n",
    "    t = (txt or \"\").lower()\n",
    "\n",
    "    out_words = [\"sold out\", \"out of stock\", \"نفدت\", \"غير متوفر\", \"غير متاح\"]\n",
    "    in_words = [\"add to cart\", \"add to bag\", \"اضف\", \"أضف\", \"متوفر\"]\n",
    "\n",
    "    for w in out_words:\n",
    "        if w in t:\n",
    "            return \"out_of_stock\", w\n",
    "\n",
    "    for w in in_words:\n",
    "        if w in t:\n",
    "            return \"in_stock\", w\n",
    "\n",
    "    return \"unknown\", None\n",
    "\n",
    "\n",
    "# =======================\n",
    "# ✅ Undetected Chrome Driver\n",
    "# =======================\n",
    "def open_driver(headless: bool = True):\n",
    "    options = uc.ChromeOptions()\n",
    "\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "\n",
    "    options.add_argument(\"--window-size=1400,900\")\n",
    "    options.add_argument(\"--lang=en-US\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    # anti-detection tweaks\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--disable-infobars\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "\n",
    "    driver = uc.Chrome(\n",
    "        options=options,\n",
    "        use_subprocess=True,\n",
    "        version_main=None\n",
    "    )\n",
    "    return driver\n",
    "\n",
    "\n",
    "def get_page_with_retry(driver, url: str, retries: int = 5, base_wait: float = 3.5) -> str:\n",
    "    last_err = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(base_wait)\n",
    "\n",
    "            # scroll يساعد lazy load\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight * 0.6);\")\n",
    "            time.sleep(1.0)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1.3)\n",
    "\n",
    "            html = driver.page_source\n",
    "            if html and \"<html\" in html.lower():\n",
    "                return html\n",
    "\n",
    "            last_err = \"HTML not loaded properly\"\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "\n",
    "        time.sleep(1.5 * attempt)\n",
    "\n",
    "    raise RuntimeError(f\"Failed to load: {url}. Last error: {last_err}\")\n",
    "\n",
    "\n",
    "def parse_products(\n",
    "    html: str,\n",
    "    category: str,\n",
    "    search_group: str,\n",
    "    search_term: str,\n",
    "    query_variant: str,\n",
    "    page_no: int\n",
    "):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    rows = []\n",
    "\n",
    "    titles = soup.select('h2[data-qa=\"plp-product-box-name\"]')\n",
    "    for h2 in titles:\n",
    "        title = h2.get(\"title\") or safe_text(h2)\n",
    "        title = clean_space(title)\n",
    "\n",
    "        # climb up to card container\n",
    "        card = h2\n",
    "        for _ in range(30):\n",
    "            if not card:\n",
    "                break\n",
    "            if card.name == \"div\" and (\n",
    "                card.select_one('[data-qa=\"plp-product-box-price\"]')\n",
    "                or card.select_one(\"a[href]\")\n",
    "            ):\n",
    "                break\n",
    "            card = card.parent\n",
    "\n",
    "        if not card:\n",
    "            continue\n",
    "\n",
    "        a = h2.find_parent(\"a\")\n",
    "        if not a:\n",
    "            a = card.select_one(\"a[href]\")\n",
    "        product_url = urljoin(BASE, a[\"href\"]) if a and a.get(\"href\") else None\n",
    "\n",
    "        # price box\n",
    "        price_box = card.select_one('[data-qa=\"plp-product-box-price\"]')\n",
    "        currency = safe_text(price_box.select_one(\"span\")) if price_box else None\n",
    "        price = safe_text(price_box.select_one(\"strong\")) if price_box else None\n",
    "        old_price = safe_text(price_box.select_one('[class*=\"oldPrice\"]')) if price_box else None\n",
    "\n",
    "        txt = clean_space(card.get_text(\" \", strip=True))\n",
    "\n",
    "        rating = None\n",
    "        m = re.search(r\"\\b(\\d\\.\\d)\\b\", txt)\n",
    "        if m:\n",
    "            rating = m.group(1)\n",
    "\n",
    "        reviews = None\n",
    "        count_span = card.select_one('[class*=\"countCtr\"] span')\n",
    "        if count_span:\n",
    "            reviews = safe_text(count_span)\n",
    "        else:\n",
    "            m2 = re.search(r\"\\b(\\d+(?:\\.\\d+)?[KkMm])\\b\", txt)\n",
    "            if m2:\n",
    "                reviews = m2.group(1)\n",
    "\n",
    "        stock_status, stock_text = detect_stock(txt)\n",
    "\n",
    "        rows.append({\n",
    "            \"date\": TODAY_STR,\n",
    "            \"category\": category,\n",
    "            \"search_group\": search_group,\n",
    "            \"search_term\": search_term,\n",
    "            \"query_variant\": query_variant,\n",
    "            \"page\": page_no,\n",
    "\n",
    "            \"title\": title,\n",
    "            \"url\": product_url,\n",
    "            \"currency\": currency,\n",
    "            \"price\": price,\n",
    "            \"old_price\": old_price,\n",
    "            \"rating\": rating,\n",
    "            \"reviews\": reviews,\n",
    "            \"stock_status\": stock_status,\n",
    "            \"stock_text\": stock_text\n",
    "        })\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def append_rows_to_csv(path: str, fieldnames: list, rows: list):\n",
    "    file_exists = os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            w.writeheader()\n",
    "        w.writerows(rows)\n",
    "\n",
    "\n",
    "def fetch_rows_for_page(\n",
    "    driver,\n",
    "    url: str,\n",
    "    category: str,\n",
    "    search_group: str,\n",
    "    search_term: str,\n",
    "    query_variant: str,\n",
    "    page_no: int,\n",
    "    tries: int = 3\n",
    "):\n",
    "    last_rows = []\n",
    "    last_html = None\n",
    "\n",
    "    for t in range(1, tries + 1):\n",
    "        last_html = get_page_with_retry(driver, url)\n",
    "        last_rows = parse_products(last_html, category, search_group, search_term, query_variant, page_no)\n",
    "\n",
    "        if len(last_rows) > 0:\n",
    "            return last_rows, last_html, True\n",
    "\n",
    "        time.sleep(1.2 * t)\n",
    "        try:\n",
    "            driver.refresh()\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(0.8)\n",
    "\n",
    "    return last_rows, last_html, False\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Main scrape (one daily file)\n",
    "# =======================\n",
    "def run_all(headless: bool = False):\n",
    "    \"\"\"\n",
    "    ✅ كل نتائج اليوم في ملف واحد: Data/YYYY-MM-DD.csv\n",
    "    ✅ الحد الأقصى للصفحات لكل SEARCH TERM (مثل oil) = MAX_PAGES_PER_TERM\n",
    "       يعني لو oil ليها variants (oil + cooking oil) هنقسّم الـ 10 صفحات عليهم بالترتيب.\n",
    "    \"\"\"\n",
    "    ensure_dir(DATA_DIR)\n",
    "\n",
    "    fieldnames = [\n",
    "        \"date\",\n",
    "        \"category\",\n",
    "        \"search_group\",\n",
    "        \"search_term\",\n",
    "        \"query_variant\",\n",
    "        \"page\",\n",
    "        \"title\",\n",
    "        \"url\",\n",
    "        \"currency\",\n",
    "        \"price\",\n",
    "        \"old_price\",\n",
    "        \"rating\",\n",
    "        \"reviews\",\n",
    "        \"stock_status\",\n",
    "        \"stock_text\"\n",
    "    ]\n",
    "\n",
    "    driver = open_driver(headless=headless)\n",
    "\n",
    "    try:\n",
    "        for category, terms in SEARCH_MAP.items():\n",
    "            for item in terms:\n",
    "                # item: [\"oil\",\"cooking oil\"] or [\"rice\"]\n",
    "                search_term = item[0]\n",
    "                variants = item  # variants list\n",
    "\n",
    "                # ✅ صفحات هذا الـ term كله (مش لكل variant)\n",
    "                pages_left_for_term = MAX_PAGES_PER_TERM\n",
    "\n",
    "                for query_variant in variants:\n",
    "                    if pages_left_for_term <= 0:\n",
    "                        print(f\"[{category}] term '{search_term}' -> reached MAX_PAGES_PER_TERM={MAX_PAGES_PER_TERM} (skip remaining variants)\")\n",
    "                        break\n",
    "\n",
    "                    # page 1\n",
    "                    first_url = BASE_SEARCH.format(page=1, query=quote_plus(query_variant))\n",
    "                    rows1, html1, ok1 = fetch_rows_for_page(\n",
    "                        driver, first_url,\n",
    "                        category=category,\n",
    "                        search_group=category,\n",
    "                        search_term=search_term,\n",
    "                        query_variant=query_variant,\n",
    "                        page_no=1,\n",
    "                        tries=3\n",
    "                    )\n",
    "\n",
    "                    if not ok1 or len(rows1) == 0:\n",
    "                        print(f\"[{category}] '{query_variant}' -> page 1 returned 0 products (skipped)\")\n",
    "                        continue\n",
    "\n",
    "                    total_pages = detect_total_pages_from_html(html1)\n",
    "\n",
    "                    # ✅ limit pages for this variant based on remaining pages for the term\n",
    "                    pages_for_this_variant = min(total_pages, pages_left_for_term)\n",
    "\n",
    "                    # save page 1\n",
    "                    append_rows_to_csv(OUT_CSV, fieldnames, rows1)\n",
    "                    pages_left_for_term -= 1\n",
    "                    print(f\"[{category}] term='{search_term}' variant='{query_variant}' page 1/{pages_for_this_variant} -> {len(rows1)} rows appended (pages_left_for_term={pages_left_for_term})\")\n",
    "\n",
    "                    if pages_for_this_variant <= 1:\n",
    "                        continue\n",
    "\n",
    "                    # باقي الصفحات بالتسلسل (حتى pages_for_this_variant)\n",
    "                    for p in range(2, pages_for_this_variant + 1):\n",
    "                        if pages_left_for_term <= 0:\n",
    "                            print(f\"[{category}] term '{search_term}' -> reached MAX_PAGES_PER_TERM={MAX_PAGES_PER_TERM} (stop term)\")\n",
    "                            break\n",
    "\n",
    "                        url = BASE_SEARCH.format(page=p, query=quote_plus(query_variant))\n",
    "                        rows, _html, ok = fetch_rows_for_page(\n",
    "                            driver, url,\n",
    "                            category=category,\n",
    "                            search_group=category,\n",
    "                            search_term=search_term,\n",
    "                            query_variant=query_variant,\n",
    "                            page_no=p,\n",
    "                            tries=3\n",
    "                        )\n",
    "\n",
    "                        if not ok or len(rows) == 0:\n",
    "                            print(f\"[{category}] term='{search_term}' variant='{query_variant}' page {p}/{pages_for_this_variant} -> 0 products (stop this variant)\")\n",
    "                            break\n",
    "\n",
    "                        append_rows_to_csv(OUT_CSV, fieldnames, rows)\n",
    "                        pages_left_for_term -= 1\n",
    "                        print(f\"[{category}] term='{search_term}' variant='{query_variant}' page {p}/{pages_for_this_variant} -> {len(rows)} rows appended (pages_left_for_term={pages_left_for_term})\")\n",
    "\n",
    "                        time.sleep(1.0)\n",
    "\n",
    "        print(f\"\\nDONE ✅ Daily file created/updated: {OUT_CSV}\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # headless=False عشان تشوفي اللي بيحصل، لو عايزة headless خليها True\n",
    "    run_all(headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
